{
 "metadata": {
  "name": "",
  "signature": "sha256:8429391fe909436f808919fc4d5d5f8d8d6fa76908f82fb7521f5ffc47a04b69"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Text Classification##\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The purpose of the text classification is to create a classifier that can classify a tweet as 'pro-protester' or 'pro-police'.  These classifications will be used to calucate the percent of 'pro-protester' tweets for specific users.  That data will then be used as a feature in a Random Forest Classification that will attempt to classify whether a user will remain engaged in the Ferguson conversion on twitter.\n",
      "\n",
      "Resource and code help: Python 3 Text Processing with NLTK 3 Cookbook, Jacob Perkins"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Imports and connect to database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#imports and query mongodb\n",
      "\n",
      "import json\n",
      "import pymongo \n",
      "from bson import json_util # From  pymongo\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "mdb = pymongo.MongoClient('mongodb://10.208.160.157')\n",
      "db = mdb.ferguson\n",
      "tweets = db.tweets\n",
      "aug_tweets = db.tweets_aug"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Obtain and prepare training data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specific users were chosen to train the classifier.  The users were chosen based on the the content of their tweets.  To be chosen for the training set, the majority of the user's Ferguson related content needed to be clearly 'pro-protester' or 'pro-police', and the user's tweets should not consist of primarily retweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#queries the database for the user's tweets and creates a dataframe for the user\n",
      "\n",
      "def getTweetsAndLabel(name, label):\n",
      "    tweet_fields = ['id', '_iso_created_at', 'text' ]\n",
      "    user = tweets.find({\"user.screen_name\": name}, fields = tweet_fields).sort([(\"_iso_created_at\", -1 )])\n",
      "    df = pd.DataFrame(list(user), columns = tweet_fields)\n",
      "    df['label'] = label\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A dataframe is generated with the tweets of each user, and each tweet is given a label based on whether that users is 'pro-protester' or 'pro-police'.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dataframes for pro-protester users: 'p'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deray_df = getTweetsAndLabel('deray', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "big6domino_df = getTweetsAndLabel('Big6domino', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wesleylowery_df = getTweetsAndLabel('WesleyLowery', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plussone_df = getTweetsAndLabel('plussone', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jsanbower_df = getTweetsAndLabel('JSanbower', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est_laced_up_df = getTweetsAndLabel('EST_Laced_Up', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lisaarmstrong_df = getTweetsAndLabel('LisaArmstrong', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "judsontwit_df = getTweetsAndLabel('judsontwit', 'p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#combine all pro-protester user dataframes\n",
      "prot = [deray_df, big6domino_df, wesleylowery_df, plussone_df, est_laced_up_df, judsontwit_df, jsanbower_df, lisaarmstrong_df]\n",
      "prot_df = pd.concat(prot)\n",
      "len(prot_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "7215"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dataframes for pro-police users: 'c'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hotnostrilsrfun_df = getTweetsAndLabel('HotNostrilsrFun', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rburg63_df = getTweetsAndLabel('rburg63', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msully65_df = getTweetsAndLabel('msully65', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anna12061_df = getTweetsAndLabel('anna12061', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scrufey21_df = getTweetsAndLabel('Scrufey21', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jake_bradford88_df = getTweetsAndLabel('jake_bradford88', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "johncardillo_df = getTweetsAndLabel('johncardillo', 'c')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#combine all pro-police dataframes\n",
      "pol = [hotnostrilsrfun_df, rburg63_df, msully65_df, anna12061_df, scrufey21_df, jake_bradford88_df, johncardillo_df]\n",
      "pol_df = pd.concat(pol)\n",
      "len(pol_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "5884"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#combine pro-protester and pro-police dataframes\n",
      "users_df = []\n",
      "df_all = pd.concat([prot_df, pol_df]).reset_index(drop = True)\n",
      "len(df_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "13099"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the balance of the data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Pro-protestor tweets make up {0:.0f}% of the labeled data set.\".format((len(prot_df)*1.0 / len(df_all))*100)\n",
      "print \"Pro-police tweets make up {0:.0f}% of the labeled data set.\".format((len(pol_df)*1.0 / len(df_all))*100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pro-protestor tweets make up 55% of the labeled data set.\n",
        "Pro-police tweets make up 45% of the labeled data set.\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Prepare the text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The text was prepared for training using the following steps:\n",
      "- tokenize tweets to words\n",
      "- convert words to lowercase\n",
      "- remove standard NLTK stop words, as these common words are not useful for classification\n",
      "- remove custom stop words, such as http, as these words are not useful for classification\n",
      "- remove words with length less than 2, as these words are not useful for classification\n",
      "- remove numbers, as they are not useful for classification-\n",
      "- include the 10 most common bigrams which were determing using the chi-square test  \n",
      "\n",
      "Note that many of these steps could be optimized further.  For instance, more or less bigrams could be used, more stop words could be added, and the analysis could be restricted to a specific number of most important words.  However, the accuracy of the classifier was high enough that further optimization of the text processing was not necessary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#prepare text using steps described above\n",
      "def prepText(df):\n",
      "    #get the NLTK english stop words\n",
      "    from nltk.corpus import stopwords\n",
      "    english_stops = set(stopwords.words('english'))\n",
      "    from nltk.collocations import BigramCollocationFinder\n",
      "    from nltk.metrics import BigramAssocMeasures\n",
      "\n",
      "    for i in range (0, len(df)):\n",
      "        \n",
      "        words = word_tokenize(df.loc[i, 'text'])\n",
      "        words = [w.lower() for w in words]\n",
      "        \n",
      "        custom_stops = ['http', 'https', '...', 'amp']\n",
      "        \n",
      "        words = [word for word in words  if word not in english_stops and len(word) > 2 and word not in custom_stops and word.isalpha()]\n",
      "        \n",
      "        bigram_finder = BigramCollocationFinder.from_words(words)\n",
      "        bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 10)\n",
      "    \n",
      "        words = (words + bigrams)\n",
      "        \n",
      "        #put the word list into the correct format for the classifier \n",
      "        words_dict = {word: True for word in words}     \n",
      "        df.loc[i, 'train data'] = ''.join(str(words_dict))\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Use parallel computing for text preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython parallel computing was used to make the text prepartion operation faster."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#set up parallel computing and confirm number of engines\n",
      "\n",
      "from IPython import parallel\n",
      "\n",
      "#make sure to enter the correct profile\n",
      "clients = parallel.Client(profile='nbserver')\n",
      "\n",
      "# use synchronous computations - all results must finish computing before any results are recorded\n",
      "clients.block = True  \n",
      "dview = clients.direct_view()\n",
      "print clients.ids\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2, 3, 4, 5]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px from nltk.tokenize import word_tokenize\n",
      "\n",
      "dview.push((dict(prepText = prepText))) \n",
      "\n",
      "dview.scatter('df_all', df_all)\n",
      "dview.execute('df_all.reset_index(drop = True, inplace = True)')\n",
      "dview.execute('df_all = prepText(df_all)')\n",
      "all_list = dview.gather('df_all')\n",
      "df_all = pd.concat(all_list).reset_index(drop = True)\n",
      "print len(df_all)\n",
      "df_all.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13099\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>_iso_created_at</th>\n",
        "      <th>text</th>\n",
        "      <th>label</th>\n",
        "      <th>train data</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 540731110779416576</td>\n",
        "      <td>2014-12-05 04:55:17</td>\n",
        "      <td> RT @PWeiskel08: Pepper balls in Boston. Lots o...</td>\n",
        "      <td> p</td>\n",
        "      <td> {u'pepper': True, u'balls': True, u'people': T...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 540730147423272960</td>\n",
        "      <td>2014-12-05 04:51:28</td>\n",
        "      <td> We do not accept Chief Belmar's \"apology\" re: ...</td>\n",
        "      <td> p</td>\n",
        "      <td> {u'belmar': True, u'tamir': True, u'murder': T...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 540729682274959360</td>\n",
        "      <td>2014-12-05 04:49:37</td>\n",
        "      <td> RT @PDPJ: #Ferguson protesters burn flag, Nati...</td>\n",
        "      <td> p</td>\n",
        "      <td> {u'remnants': True, u'national': True, u'walk'...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 540727520530661376</td>\n",
        "      <td>2014-12-05 04:41:01</td>\n",
        "      <td> Why protest? Because we refuse to be scared in...</td>\n",
        "      <td> p</td>\n",
        "      <td> {u'refuse': True, (u'refuse', u'drown'): True,...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 540726405986652160</td>\n",
        "      <td>2014-12-05 04:36:36</td>\n",
        "      <td> Issue #64 of the #Ferguson Protestor Newslette...</td>\n",
        "      <td> p</td>\n",
        "      <td> {(u'share', u'stay'): True, (u'protestor', u'n...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "                   id     _iso_created_at  \\\n",
        "0  540731110779416576 2014-12-05 04:55:17   \n",
        "1  540730147423272960 2014-12-05 04:51:28   \n",
        "2  540729682274959360 2014-12-05 04:49:37   \n",
        "3  540727520530661376 2014-12-05 04:41:01   \n",
        "4  540726405986652160 2014-12-05 04:36:36   \n",
        "\n",
        "                                                text label  \\\n",
        "0  RT @PWeiskel08: Pepper balls in Boston. Lots o...     p   \n",
        "1  We do not accept Chief Belmar's \"apology\" re: ...     p   \n",
        "2  RT @PDPJ: #Ferguson protesters burn flag, Nati...     p   \n",
        "3  Why protest? Because we refuse to be scared in...     p   \n",
        "4  Issue #64 of the #Ferguson Protestor Newslette...     p   \n",
        "\n",
        "                                          train data  \n",
        "0  {u'pepper': True, u'balls': True, u'people': T...  \n",
        "1  {u'belmar': True, u'tamir': True, u'murder': T...  \n",
        "2  {u'remnants': True, u'national': True, u'walk'...  \n",
        "3  {u'refuse': True, (u'refuse', u'drown'): True,...  \n",
        "4  {(u'share', u'stay'): True, (u'protestor', u'n...  "
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Generate training and test data sets\n",
      "\n",
      "The data is split into random training and test sets using sklearn.cross_validation.train_test_split, with a test size of 0.33.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#prepare train and test sets\n",
      "import ast\n",
      "import collections\n",
      "from nltk import metrics\n",
      "data_list = [] * len(df_all)\n",
      "\n",
      "def prepTrainText(df):\n",
      "    import numpy as np\n",
      "    import sklearn\n",
      "    from sklearn.cross_validation import train_test_split\n",
      "    \n",
      "    #convert strings in dataframe to dicts\n",
      "    for i in range(0, len(df)):\n",
      "        data_list.append((ast.literal_eval(df.loc[i]['train data']), df.loc[i]['label']))\n",
      "    train_data, test_data = sklearn.cross_validation.train_test_split(data_list, test_size = 0.33,random_state = 42)\n",
      "    return train_data, test_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set, test_set = prepTrainText(df_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The length of of the training set is %d.\" % len(train_set)\n",
      "print \"The length of the test set is %d.\" % len(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The length of of the training set is 8776.\n",
        "The length of the test set is 4323.\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Train and test the classifier\n",
      "\n",
      "Since this is a text classification problem we chose to train a NLTK Naive Bayes classifier.  Naive Bayes is suggested as good starting point in the Python 3 Text Processing with NLTK 3 Cookbook.  \n",
      "\n",
      "In addition, the scikit-learn algorithm cheat sheet (http://scikit-learn.org/stable/tutorial/machine_learning_map/) shows Naive Bayes as a good choice for labeled text data.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train classifier and show most informative features\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "classifier = NaiveBayesClassifier.train(train_set)\n",
      "classifier.show_most_informative_features()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "                 rioters = True                c : p      =     57.6 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   (u'ferguson', u'cnn') = True                p : c      =     47.5 : 1.0\n",
        "                    thug = True                c : p      =     39.6 : 1.0\n",
        "(u'movement', u'ferguson') = True                p : c      =     36.2 : 1.0\n",
        "            conservative = True                c : p      =     29.0 : 1.0\n",
        "                  stolen = True                c : p      =     27.4 : 1.0\n",
        " (u'protest', u'leader') = True                c : p      =     25.7 : 1.0\n",
        "              terrorists = True                c : p      =     23.3 : 1.0\n",
        "                tchopstl = True                p : c      =     20.4 : 1.0\n",
        "                    west = True                p : c      =     19.3 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to calculate precision and recall\n",
      "def precAndRec(classifier, test_feats):\n",
      "    refsets = collections.defaultdict(set)\n",
      "    testsets = collections.defaultdict(set)\n",
      "\n",
      "    precisions = {}\n",
      "    recalls = {}\n",
      "    for i, (feats, label) in enumerate(test_feats):\n",
      "        refsets[label].add(i)\n",
      "        observed = classifier.classify(feats)\n",
      "        testsets[observed].add(i)\n",
      "    \n",
      "    for label in classifier.labels():\n",
      "        precisions[label] = metrics.precision(refsets[label], testsets[label])\n",
      "        recalls[label] = metrics.recall(refsets[label], testsets[label])\n",
      "    \n",
      "    return precisions, recalls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test classifier and output results\n",
      "import nltk.classify.util\n",
      "print 'Accuracy: %0.2f' % nltk.classify.util.accuracy(classifier, test_set)\n",
      "\n",
      "prec, rec = precAndRec(classifier, test_set)\n",
      "print \"Precision for 'p': %0.2f\" % prec['p']\n",
      "print \"Precision for 'c': %0.2f\" % prec['c']\n",
      "print \"Recall for 'p': %0.2f\" % rec['p']\n",
      "print \"Recall for 'c': %0.2f\" % rec['c']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.84\n",
        "Precision for 'p': 0.88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision for 'c': 0.79\n",
        "Recall for 'p': 0.82\n",
        "Recall for 'c': 0.87\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The accuracy, precision, and recall are all good.  Therefore, no further work on text optmization will be done."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn also offers a Naive Byes classifier, and NLTK provides a wrapper class for it.  Scikit-learn's Naive Bayes classifier has a smaller memory footprint.  With the amount of data we're working with, memory considerations are important  So, we also tried this classifier. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sklearn Naive Bayes (usually better than NLTK, but slower)\n",
      "from nltk.classify import scikitlearn\n",
      "\n",
      "from nltk.classify.scikitlearn import SklearnClassifier\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "sknb_classifier = SklearnClassifier(MultinomialNB())\n",
      "sknb_classifier.train(train_set)\n",
      "print nltk.classify.util.accuracy(sknb_classifier, test_set)\n",
      "\n",
      "prec, rec = precAndRec(sknb_classifier, test_set)\n",
      "print \"Precision for 'p': %0.2f\" % prec['p']\n",
      "print \"Precision for 'c': %0.2f\" % prec['c']\n",
      "print \"Recall for 'p': %0.2f\" % rec['p']\n",
      "print \"Recall for 'c': %0.2f\" % rec['c']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.848947490169\n",
        "Precision for 'p': 0.84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision for 'c': 0.87\n",
        "Recall for 'p': 0.90\n",
        "Recall for 'c': 0.79\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results are similiar.  Because of the memory footprint, the sklearn Naive Bayes will be used for the  classifications.\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Classification sanity check"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To confirm that the classifier is classifying tweets properly, additional users were identified as clearly pro-protester or pro-police.  Their tweets were classified, and the results are below.\n",
      "\n",
      "Note: 'p' = 'pro-protester'\n",
      "      'c' = 'pro-police'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#determine percent of user's tweets that are pro-protester\n",
      "def testUser(name, classifier):\n",
      "    df = getTweetsAndLabel(name, '')\n",
      "    df_ready = prepText(df)\n",
      "    for i in range(0, len(df_ready)):\n",
      "        df_ready.loc[i, 'label'] = classifier.classify(ast.literal_eval(df_ready.loc[i]['train data']))\n",
      "    p = len(df_ready[df_ready['label'] == 'p'])\n",
      "    c = len(df_ready[df_ready['label'] == 'c'])\n",
      "    t = p + c\n",
      "    perc_p = p*1.0 / t\n",
      "    return perc_p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('AntonioFrench', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'p'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.826251896813\n",
        "Expected: 'p'\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('FredSanford13', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'c'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.313455657492\n",
        "Expected: 'c'\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('CAC8438', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'c'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.194331983806\n",
        "Expected: 'c'\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('OpFerguson', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'p'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.843704245974\n",
        "Expected: 'p'\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('ryanjreilly', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'p'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.820408163265\n",
        "Expected: 'p'\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('Nettaaaaaaaa', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'p'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.911217437533\n",
        "Expected: 'p'\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('timjacobwise', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'p'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.822222222222\n",
        "Expected: 'p'\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('1969WAR1971', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'c'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.386363636364\n",
        "Expected: 'c'\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUser('rdipego', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'c'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.472222222222\n",
        "Expected: 'c'\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each user was classified as expected.  If 'p' was expected, the percent of pro-protester tweets should be above 0.5.  If 'c' was expected, it should be below 0.5."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Notes on the data used to train the text classifier:\n",
      "\n",
      "In other parts of this project, restrictions are being made to the data that have not been made to the data set used to train the text classifier.  These restrictions include only looking at tweets with the hashtag #Ferguson or #ferguson (versus containing the word F/ferguson, even if not as a hashtag), and restricting the data to tweets from August.\n",
      "\n",
      "For the purpose of text classification, the context of the tweets is the most important factor.  The presence of #Ferguson or #ferguson hastags is not important, nor is the date of the tweets.  The context of the tweets remains largely the same through the time period being analysed.  The high accuracy, precision, and recall of the classifier confirms that these restrictions of the data are not necessary for text classification.\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Prepare for classification of the users in the August data set\n",
      "\n",
      "Each of the users in the August data will have their tweets classified, and the percent of their tweets that are pro-protester will be calculated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get users from August data\n",
      "df_aug = pd.read_csv('/home/data/august_reduced.csv', error_bad_lines = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to get tweets for each user using August data set\n",
      "def getTweetsAndLabelAug(name, label):\n",
      "    df_users_tweets = df_aug[df_aug['user.screen_name'] == name].reset_index(drop = True)\n",
      "    df_users_tweets ['label'] = label\n",
      "    return df_users_tweets "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "def testUserAug(name, classifier):\n",
      "    df = getTweetsAndLabelAug(name, '')\n",
      "    df_ready = prepText(df)\n",
      "    for i in range(0, len(df_ready)):\n",
      "        df_ready.loc[i, 'label'] = classifier.classify(ast.literal_eval(df_ready.loc[i]['train data']))\n",
      "    p = len(df_ready[df_ready['label'] == 'p'])\n",
      "    c = len(df_ready[df_ready['label'] == 'c'])\n",
      "    t = p + c\n",
      "    perc_p = p*1.0 / t\n",
      "    return perc_p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A few additional users identified during EDA were tested as a sanity check."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUserAug('gerfingerpoken2', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'c'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.000337268128162\n",
        "Expected: 'c'\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUserAug('carolynsbuddy', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: 'p'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.847727272727\n",
        "Expected: 'p'\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = testUserAug('PhilDeCarolis', sknb_classifier)\n",
      "print x\n",
      "print \"Expected: '?'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.721700717835\n",
        "Expected: '?'\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "@PhilDeCarolis is interesting.  He appears to be a liberatarian and supporter of Ron Paul.  Most of his tweets regarding Ferguson are retweets of news headlines.  He doesn't seem to outright support either the protestors or the police. \n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Classify tweets from August\n",
      "\n",
      "The tweets of 5,000 random users with ten or more tweets in August will be classified.  The sample number of 5,000 was chosen based on time and server memory constraints.  The set of tweets from 5,000 users would take about 2 - 2.5 hours to classify, and would occasionally cause the IPython kernel to crash.  Larger servers were tried, and 5,000 was the limit to classify in a reasonable amount of time without kernel crashes on the largest server available to the team."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython's parallel computing would have been nice to use for this lengthy process.  However, when we attempted to scatter the work to the available cores, the process was stopped by memory errors.  This may have been caused by the amount of data that needed to be pushed to each core to run the process.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Generate dataframe with users from August\n",
      "users = pd.DataFrame({'count' : df_aug.groupby( [ \"user.screen_name\"] ).size()}).reset_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#restrict to users with tweet counts greater than 10, and randomly select 5000\n",
      "users = users[pd.notnull(users['count'])]\n",
      "users = users[users['count'] >= 10]\n",
      "rows = np.random.choice(users.index.values, 5000, replace=False)\n",
      "ran_users = users.ix[rows].reset_index(drop = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(ran_users)\n",
      "ran_users.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5000\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>user.screen_name</th>\n",
        "      <th>count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>      ItriSamele</td>\n",
        "      <td> 11</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>      gohogsgirl</td>\n",
        "      <td> 28</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>         averrer</td>\n",
        "      <td> 17</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>     17147578976</td>\n",
        "      <td> 15</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> fucking_ninoX_X</td>\n",
        "      <td> 12</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "  user.screen_name  count\n",
        "0       ItriSamele     11\n",
        "1       gohogsgirl     28\n",
        "2          averrer     17\n",
        "3      17147578976     15\n",
        "4  fucking_ninoX_X     12"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to classify tweets and caluclate percent per user\n",
      "def label_all(df):\n",
      "    for i in range(0, len(df)):\n",
      "        x = testUserAug(df.loc[i]['user.screen_name'], sknb_classifier)\n",
      "        df.loc[i, 'perc_p'] = x\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#classify tweets for users\n",
      "final = label_all(ran_users)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "final.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>user.screen_name</th>\n",
        "      <th>count</th>\n",
        "      <th>perc_p</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>      ItriSamele</td>\n",
        "      <td> 11</td>\n",
        "      <td> 0.727273</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>      gohogsgirl</td>\n",
        "      <td> 28</td>\n",
        "      <td> 0.857143</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>         averrer</td>\n",
        "      <td> 17</td>\n",
        "      <td> 0.823529</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>     17147578976</td>\n",
        "      <td> 15</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> fucking_ninoX_X</td>\n",
        "      <td> 12</td>\n",
        "      <td> 0.833333</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 86,
       "text": [
        "  user.screen_name  count    perc_p\n",
        "0       ItriSamele     11  0.727273\n",
        "1       gohogsgirl     28  0.857143\n",
        "2          averrer     17  0.823529\n",
        "3      17147578976     15  1.000000\n",
        "4  fucking_ninoX_X     12  0.833333"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(final)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "5000"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save dataframe for use in another notebook\n",
      "final.to_pickle('final_aug_percents.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculate number of users that are pro-protester and number that are pro-police\n",
      "p = len(final[final['perc_p'] >= 0.5])\n",
      "c = len(final[final['perc_p'] < 0.5])\n",
      "\n",
      "print \"Percent of users that are pro-protester: {0:.0f}%\".format(p*1.0/(p+c)*100)\n",
      "print \"Percent of users that are pro-police: {0:.0f}%\".format(c*1.0/(c+p)*100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Percent of users that are pro-protester: 91%\n",
        "Percent of users that are pro-police: 9%\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "---"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}